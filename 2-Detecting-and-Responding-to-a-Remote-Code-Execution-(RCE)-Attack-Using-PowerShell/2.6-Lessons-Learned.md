## ðŸ“˜ Lessons Learned

- This project was a valuable exercise in understanding how PowerShell can be both a legitimate administrative tool and a potential vector for attack. Crafting the simulated RCE scenario helped me appreciate the subtle ways threat actors can use native tools to evade traditional detection methods.

- Creating a custom detection rule using KQL was challenging at first. Fine-tuning the logic to distinguish malicious behaviour from normal administrative activity reinforced the importance of precision in detection engineering.

- One of the key lessons was the power and limitations of automated response. While isolating the VM and collecting forensic packages occurred seamlessly, reviewing the investigation results still required manual effort and interpretationâ€”especially when navigating through process trees and event timelines.

- Analysing the forensic data taught me that real-world incident response often involves a great deal of contextual understanding. Seeing a suspicious PowerShell command in isolation isnâ€™t always enough; itâ€™s the surrounding activity and user behaviour that complete the picture.

- I also learned how useful the Action Centre is for tracking automated responses, but I noticed there can be a delay between alert detection and response execution. This highlighted the need to manage expectations around automation timing in live environments.

- In retrospect, I would improve the project by expanding the range of simulated behaviours (e.g. post-exploitation activity or lateral movement) to test the robustness of detection rules beyond the initial access phase.

Overall, this project reinforced key SOC principles such as layered defence, proactive monitoring, and structured investigation. It also made clear that effective detection and response requires a blend of automation, scripting, and careful analysis.
